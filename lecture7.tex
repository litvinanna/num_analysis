\section{Оптимимзация и методы решения систем нелинейных уравнений}
\label{lecture7}

Начнем с примера. Пусть, как и в МНК, зависимость $\overline{f}(t)$ ищется из условия (см. \ref{eq:3.5})
\begin{equation} \label{eq:7.1}
	Q(x_1, x_2, x_3, x_4) = \sum^m_{i=1}{(\overline{f}(t_i)-b_i)^2} = \min_{x_1 \dots x_4}.
\end{equation}

Но теперь в отличие от (\ref{eq:5.5}) функция $\overline{f}(t)$ зависит от $x_3$, $x_4$ нелинейно
\begin{equation} \label{eq:7.2}
	\overline{f}(t) = x_1e^{x_3 t} + x_2e^{x_4 t}.
\end{equation}
Тогда условие (\ref{eq:7.1}) приводит к нелинейной системе уравнений
\begin{equation} \label{eq:7.3}
	f_i(x) = 0 \quad
	f_i(x) \equiv f_i(x_1, x_2, x_3, x_4) = \frac{\partial Q}{\partial x_i}(x).
\end{equation}

\textbf{Задача оптимизации} состоит в поиске минимума величины 
\begin{equation} \label{eq:7.4}
	Q(x) \equiv Q(x_1 \dots x_4), Q(x) > 0,
\end{equation}
которая называется \textbf{ценой}. Как правило на $x$ накладываются условия вида $x \in U$, где область $U$ определяется условиями
\begin{equation} \label{eq:7.5}
	a_i < x_i < b_i \textrm{или} \|x\| < R.
\end{equation}

Это вносит дополнительные трудности, так как минимум $Q(x)$ может достигаться на границе $\partial U$  области $U \subset \mathbb{R}^n$. Этот случай надо исследовать отдельно, и мы его рассматривать не будем. Таким образом, предполагается, что минимум $Q(x)$ достигается во внутренней точке области $U$ и тогда задача оптимизации сводится к решению системы нелинейных уравнений
\begin{equation} \label{eq:7.6}
	f_i(x) = \frac{\partial Q}{\partial x_i}(x) = 0 \quad x = (x_1 \dots x_4), i = 1 \dots n.
\end{equation}

И обратно, задача решения системы уравнений
\begin{equation} \label{eq:7.7}
	f_i(x) = 0, i = 1 \dots n.
\end{equation}
сводится к задаче оптимизации. Для этого достаточно положить
\begin{equation} \label{eq:7.8}
	Q(x) = \sum^n_{i=1}{f_i(x)^2}.
\end{equation}

В этой лекции рассматривается задача решения системы уравнений (\ref{eq:7.7}). Это трудная задача. Все методы ее решения итерационные и имеют ограниченную область применения. 

\textbf{Общая схема итерационных} методов решения системы (\ref{eq:7.7}) (методов последовательных приближений) состоит в следующем.

Выбираются некоторое нулевое приближение $x^{(0)} = (x_1^{(0)} \dots x_4^{(0)})$ и задается алгоритм $A$ построения следующих приближений $x^{(n)}$: $x^{(n)} = A(x^{(n-1)})$, $n \geq 1$. В области $U \subset \mathbb{R}^n$ считается заданной норма $\|x\| (x \in U)$ и пара $(x^{(0)}, A)$ должны быть выбраны так, что последовательность $x^{(n)}$ сходится к решению $x = x_0 = (x_{01} \dots x_{0n})$ системы (\ref{eq:7.7}), которую мы будем записывать в виде
\begin{equation} \label{eq:7.9}
	f(x) = 0, [f(x) \in \mathbb{R}^n \quad f(x) = (f_1(x) \dots f_n(x))].
\end{equation}

Таким образом должно выполняться условие сходимости
\begin{equation} \label{eq:7.10}
	\|x^{(n)} - x_0\| \xrightarrow[n \to \infty]{} 0.
\end{equation}

Ниже рассматриваются два метода решения системы $f(x) = 0$ - \textbf{метод неподвижной точки} и \textbf{метод редукции к одномерному случаю}.

Рассмотрим метод неподвижной точки.

У нас задано отображение
\begin{equation} \label{eq:7.11}
	f: U \subset \mathbb{R}^n \to \mathbb{R}^n, (f(x) = (f_1(x) \dots f_n(x))).
\end{equation}

Запишем условие $f(x) = 0$ в виде
\begin{equation} \label{eq:7.12}
	g(x) = x \textrm{, где } g(x) = x - f(x).
\end{equation}
Так как равенство $g(x_0) = x_0$ влечет $f(x_0) = 0$, то решение системы (\ref{eq:7.9}) - это неподвижная точка отображения $g: U \to \mathbb{R}^n$.

Отображение $g: U \to \mathbb{R}^n$ называется \textbf{сжимающим} в шаре $D_a(R)$ с цетром $a$ и радиусом $R$ ($\|x - a\| < R, x \in D_a(R)$).

$D_a(R) \subset U$, если для любых двух точек $x, y \in D_a(R)$ выполняется условие сжатия
\begin{equation} \label{eq:7.13}
	\|g(x) - g(y)\| < q\|x - y\|, q < 1.
\end{equation}
Если это условие выполняется, то алгоритм построения последовательных приближений $x^{(n)}$ состоит в следующем. Выбирается любая точка $x^{(0)} \in D_a(R)$ (например $x^{(0)} = a$) и $x^{{n}}$ строится по формуле
\begin{equation} \label{eq:7.14}
	x^{(n)} = g(x^{(n-1)}), n \geq 1.
\end{equation}
Так как
\begin{equation} \label{eq:7.15}
	\|x^{(n+1)} - x^{(n)}\| = \|g(x^{(n)}) - g(x^{(n-1)})\| \leq q\|x^{(n)} - x^{(n-1)}\|,
\end{equation}
то последовательность $x^{(n)}$ сходится к решению $x_0 (f(x_0) = 0)$ и 
\begin{equation} \label{eq:7.16}
	\|x^{(n)} - x_0\| \leq \frac{q^nR}{(1-q)}.
\end{equation}


Основная трудность при применении метода неподвижной точки состоит в определении области сжатия $D_a(R)$. Система (\ref{eq:7.9}) может иметь не одно решение и рассматриваемый алгоритм сходится только если нулевое приближение $x^{(0)}$ выбрано достаточно близко к одному из решений $x_0$.

Переходим к рассмотрению \textbf{метода редукции к одномерному случаю}. Будем предполагать, что мы умеем решать одно нелинейное уравнение с одним неизвестным.

Рассмотрим систему (\ref{eq:7.7}). Эта система уравнений вида
\begin{equation} \label{eq:7.17}
	\begin{cases} 
		f_1(x_1, x_2 \dots x_n) = 0 \\
		f_2(x_1, x_2 \dots x_n) = 0 \\
		\dots \\
		f_n(x_1, x_2 \dots x_n) = 0 \\
	\end{cases}.
\end{equation}
Пусть выбарно нулевое приближение $x^{(0)} = (x_1^{(0)} \dots x_n^{(0)})$. Ищем первое приближение для величины $x_1$ из уравнения
\begin{equation} \label{eq:7.18}
	f_1(x_1, x_2^{(0)} \dots x_n^{(0)}) = 0.
\end{equation}
Заметим, что нумерация уравнений и переменных $x_i$ произвольна и $f_1$ выбирается из условия простоты решения уравнения (\ref{eq:7.18}). Это же относится и к выбору $f_2$ и так далее.

Пусть $x_1^{(1)}$ решение этого уравнения. Если оно не одно, то выбирается одно из решений. Первое приближение для $x_2$ определяется как решение уравнения
\begin{equation} \label{eq:7.19}
	f_2(x_1^{(1)}, x_2, x_3^{(0)} \dots x_n^{(0)}) = 0,
\end{equation} 
а $x_3^{(1)}$ как решение уравнения
\begin{equation} \label{eq:7.20}
	f_3(x_1^{(1)}, x_2^{(1)}, x_3, x_4^{(0)} \dots x_n^{(0)}) = 0 \textrm{ и так далее}.
\end{equation} 
В результате получаем первое приближение
\begin{equation} \label{eq:7.21}
	x^{(1)} = A(x^{(0)}).
\end{equation} 
Приближение $x^{(n)}$ получается тем же процессом
\begin{equation} \label{eq:7.22}
	x^{(n)} = A(x^{(n-1)}), n \geq 1.
\end{equation} 
Таким образом, на каждом шаге необходимо решить только одно уравнение
\begin{equation} \label{eq:7.23}
	f(x) = 0, x \in [a, b].
\end{equation} 

Рассмотрим методы решения такого уравнения. Метод \textbf{деления отрезка} $[a, b]$ пополам позволяет локализовать решение (корень уравнения (\ref{eq:7.23})). Если величины $\varphi(a)$ и $\varphi(b)$ имеют различные знаки, то в $[a, b]$ содержится нечетное число корней. Если знаки $\varphi(a)$ и $\varphi(b)$ совпадают, то в интервале $[a, b]$ либо нет корней, либо их число четно. Делим отрезок $[a, b]$ пополам. Получаем два отрезка $[a, \frac{a+b}{2}], [\frac{a+b}{2}, b]$ длины $\frac{b-a}{2}$. В каждом из них повторяем ту же процедуру, то есть сравниваем знаки $\varphi(a)$ и $\varphi(\frac{a+b}{2})$ и знаки $\varphi(\frac{a+b}{2}), \varphi(b)$.

Дальше, каждый из интервалов $[a, \frac{a+b}{2}]$ и $[\frac{a+b}{2}, b]$ делится пополам и этот процесс продолжается. В результате получается локализация корней с точностью порядка $M_1 \frac{b-a}{2^n}$ (для $\varphi \in M_1[a, b]$). 

Чтобы достичь заданной точности этот процесс можно ускорить с помощью \textbf{метода Ньютона}.

Пусть известно, что в интервале $[a', b']$ есть один корень уравнения $\varphi(x) = 0$ и извечтно, что в этом интервале $(\varphi^{(1)}(x) \neq 0)$ ($\varphi(x)$ монотонна). Исходную локализацию корней можно построить просто рассмотрев график функции $y = \varphi(x)$. Выберем некоторую точку $x^{(0)} \in [a', b']$. Так как в рассматриваемом интервале 
\begin{equation} \label{eq:7.24}
	f(x) \approx f(x^{(0)}) + f^{(1)}(x^{(0)})(x-x^{(0)}),
\end{equation} 
то определяем $x^{(1)}$ из уравнения
\begin{equation} \label{eq:7.25}
	x^{(1)} = x^{(0)} - \frac{f(x^{(0)})}{f^{(1)}(x_0)}.
\end{equation}
Приближение $x^{(n)}$ в методе Ньютона определяется из уравнения
\begin{equation} \label{eq:7.26}
	x^{(n)} = x^{(n-1)} - \frac{f(x^{(n-1)})}{f^{(1)}(x^{(n-1)})}, n \geq 1.
\end{equation} 

Метод Ньютона сходится очень быстро. Если $x_0$ корень уравнения $\varphi(x) = 0$, то
\begin{equation} \label{eq:7.27}
	|x^{(n)} - x_0| \lesssim |x^{(0)} - x_0|^{2^n}
\end{equation} 

Заметим, что всегда имеется опасность при локализации корней пропустить два близко расположенных коррня или кратный корень $x_0$, для которого $\varphi^{(1)}(x_0) = 0$.

Метод Ньютона имеет многомерное обобщение. Из формулы Тейлора следует, что 
\begin{equation} \label{eq:7.28}
	f_i(x_1 + \eta_1 \dots x_n + \eta_n) = f_i(x) + \sum^n_{j=1}{\frac{\partial f_i}{\partial x_j}(x) \eta_j + \mathcal{O}(\|\eta\|^2)}.
\end{equation} 
Введем матрицу
\begin{equation} \label{eq:7.29}
	A^{(x)} \gets M_{n \times n} \quad (A^{(x)}_{ij} = \frac{\partial f_i}{\partial x_j}(x))
\end{equation} 
и предположим, что в рассматриваемой области $detA(x) \neq 0$. Равенство (\ref{eq:7.28}) в матричных обозначениях имеет вид
\begin{equation} \label{eq:7.30}
	f(x + \eta) = f(x) + A(x)\eta + \mathcal(O)(\|\eta\|^2).
\end{equation} 
По аналогии с одномерным случаем в многомерном методе Ньютона выбирается нулевое приближение $x^{(0)} = (x^{(0)}_1 \dots x^{(0)}_n)$ и приближение $x^{(n)}, n \geq 1$ определяется равенством
\begin{equation} \label{eq:7.31}
	x^{(n)} = x^{(n-1)} - A^{-1}(x^{(n)})f(x^{(n-1)}), n \geq 1.
\end{equation} 
Здесь $A^{-1}$ матрица обратная к $A$.